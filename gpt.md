# ü§ñ **Configuraci√≥n de IA Local ‚Äì GPT4All (LinkWich-Monitor)**

Esta gu√≠a muestra c√≥mo **instalar GPT4All**, descargar un **modelo `.gguf`**, y configurarlo en **LinkWich-Monitor** para funciones como explicaci√≥n de logs, textos, etc.

---

## 1Ô∏è‚É£ Requisitos

- **Windows 10/11** (tambi√©n funciona en Windows Server).
- **CPU x64** (GPT4All corre en CPU; GPU opcional).
- **Espacio en disco:** 2‚Äì10 GB seg√∫n el modelo.
- **RAM sugerida:** 8 GB m√≠nimo (mejor con 16 GB+).

---

## 2Ô∏è‚É£ Instalar GPT4All (Desktop + CLI tools)

1. Descarga el instalador oficial: **[gpt4all.io](https://gpt4all.io/)** ‚ûú *Download for Windows*.  
2. Ejecuta el instalador y **marca ‚ÄúCLI tools‚Äù** durante la instalaci√≥n.  
3. Al finalizar, se crear√° la carpeta (Windows):
> En esa carpeta se guardar√°n los modelos `.gguf`.


---

## 3Ô∏è‚É£ Descargar un modelo (.gguf)

Tienes dos opciones:

**A) Desde la app GPT4All**
1. Abre **GPT4All**.
2. Ve a **Model Explorer** y busca un modelo ‚Äú**Instruct**‚Äù.
3. Pulsa **Download**.

**B) Descarga directa (avanzado)**
- Descarga el archivo `.gguf` del modelo elegido y col√≥calo dentro de:
- 
### Modelos recomendados (CPU)
| **Modelo**                                | **Tama√±o aprox.** | **Uso t√≠pico**                         |
|-------------------------------------------|-------------------|----------------------------------------|
| `Phi-3-mini-4k-instruct.Q4_0.gguf`        | Peque√±o           | R√°pido, respuestas cortas y utilitarias|
| `Mistral-7B-Instruct.Q4_0.gguf`           | Mediano           | Equilibrio calidad/velocidad           |
| `Llama-3.1-8B-Instruct.Q4_0.gguf`         | Mediano‚Äìgrande    | Mejor calidad, m√°s RAM requerida       |

> **Nota sobre cuantizaci√≥n (Q2/Q3/Q4/Q5):** valores **m√°s bajos** ‚áí m√°s r√°pido/ligero, **menos** precisi√≥n; **Q4_0** es un buen punto de partida.

---

## 4Ô∏è‚É£ Configurar en LinkWich-Monitor

En **Ajustes ‚Üí IA GPT4All**:

- **IA habilitada:** activar.  
- **Ruta del modelo:** carpeta donde est√°n los `.gguf`, por ejemplo:

** C:\Users\manager\AppData\Local\nomic.ai\GPT4All

- **Nombre del modelo:** nombre **exacto** del archivo, p. ej.:
** Phi-3-mini-4k-instruct.Q4_0.gguf

- **# M√°x tokens:** l√≠mite de salida (ej. `100`‚Äì`256` para respuestas cortas).

Pulsa **Guardar cambios** y luego **Recargar**.

---

## 5Ô∏è‚É£ Prueba r√°pida

1. Usa alguna funci√≥n de IA (p. ej., explicaci√≥n de logs en Syslog).  
2. Verifica que las respuestas se generen sin errores.  
3. Si falla, revisa que el **archivo** exista en la **ruta** configurada.

---

## 6Ô∏è‚É£ Soluci√≥n de problemas

| **Problema**                               | **Causa**                                      | **C√≥mo resolver**                                                                 |
|-------------------------------------------|-----------------------------------------------|-----------------------------------------------------------------------------------|
| ‚ÄúModel file not found‚Äù                    | Ruta o nombre incorrectos                      | Copia el **path** desde el Explorador y pega el **nombre exacto** del `.gguf`.   |
| Respuestas muy lentas                     | Modelo grande / poca RAM                       | Prueba un modelo **Q4_0** menor (p. ej. `Phi-3-mini-4k-instruct.Q4_0.gguf`).     |
| Error de permisos al leer el archivo      | Carpeta protegida                              | Usa la ruta del **usuario** (AppData\Local\nomic.ai\GPT4All).                     |
| La app pide `libllmodel.dll` (al empaquetar) | Librer√≠a no usada en tu flujo                  | Tu implementaci√≥n con `gpt4all` **no requiere** esa DLL; puedes **omitirla**.     |
| La app no ‚Äúve‚Äù el modelo                  | Modelo en carpeta distinta                     | Mueve el `.gguf` a la carpeta configurada o ajusta la ruta en ajustes.           |

---

## 7Ô∏è‚É£ Buenas pr√°cticas

- Mant√©n **1‚Äì2 modelos** estables (evita tener muchos pesados).  
- Sube el **l√≠mite de tokens** solo cuando necesites respuestas largas.  
- Actualiza modelos peri√≥dicamente desde el **Model Explorer** de GPT4All.  
- Si usas port√°tiles, conecta a corriente: el CPU se estrangula en modo bater√≠a.

---

## 8Ô∏è‚É£ Campos de configuraci√≥n (resumen)

| **Campo**          | **Ejemplo**                                                      | **Descripci√≥n**                                  |
|--------------------|------------------------------------------------------------------|--------------------------------------------------|
| IA habilitada      | Activado                                                         | Enciende la IA local                             |
| Ruta del modelo    | `C:\Users\manager\AppData\Local\nomic.ai\GPT4All`                | Carpeta donde residen los `.gguf`                |
| Nombre del modelo  | `Phi-3-mini-4k-instruct.Q4_0.gguf`                               | Archivo a cargar                                 |
| # M√°x tokens       | `100`                                                            | M√°ximo de tokens de **salida** por respuesta     |

---

## 9Ô∏è‚É£ Referencias √∫tiles

- Sitio oficial: **https://gpt4all.io/**  
- Preguntas frecuentes y modelos: dentro de la app **GPT4All ‚Üí Model Explorer**.




